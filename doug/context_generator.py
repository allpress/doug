"""
Context generation for Claude and other AI assistants.

Transforms Doug's indexed repository data into optimized context documents,
CLAUDE.md files, and architecture maps.

I turn your messy multi-repo sprawl into something an AI can actually
understand. Think of me as the translator between chaos and context.
"""

import logging
from collections import Counter, defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

from doug import __version__
from doug.ai_query import AIQueryTool
from doug.config import DougConfig

logger = logging.getLogger(__name__)

# Build commands by detected build system type
_BUILD_COMMANDS: Dict[str, Dict[str, str]] = {
    "gradle": {
        "build": "./gradlew build",
        "test": "./gradlew test",
        "run": "./gradlew bootRun  # if Spring Boot",
    },
    "maven": {
        "build": "mvn clean install",
        "test": "mvn test",
        "run": "mvn spring-boot:run  # if Spring Boot",
    },
    "npm": {
        "build": "npm install && npm run build",
        "test": "npm test",
        "run": "npm start",
    },
    "pip": {
        "build": "pip install -e .",
        "test": "pytest",
        "run": "python -m <module>",
    },
    "go": {
        "build": "go build ./...",
        "test": "go test ./...",
        "run": "go run .",
    },
    "cargo": {
        "build": "cargo build",
        "test": "cargo test",
        "run": "cargo run",
    },
}

# Test frameworks detectable from build dependencies
_TEST_FRAMEWORKS = {
    "junit": "JUnit",
    "testng": "TestNG",
    "pytest": "pytest",
    "unittest": "unittest",
    "jest": "Jest",
    "mocha": "Mocha",
    "vitest": "Vitest",
    "testing": "Go testing",
    "rspec": "RSpec",
    "phpunit": "PHPUnit",
}

# Linter config files to detect
_LINTER_FILES = {
    ".eslintrc", ".eslintrc.js", ".eslintrc.json", ".eslintrc.yml",
    ".prettierrc", ".prettierrc.json", ".prettierrc.yml",
    "ruff.toml", ".flake8", ".pylintrc", "pylintrc",
    ".rubocop.yml", ".golangci.yml", "clippy.toml",
    "biome.json", "deno.json",
}

# Language name mapping from file extension
_LANG_NAMES: Dict[str, str] = {
    ".py": "Python", ".java": "Java", ".kt": "Kotlin",
    ".js": "JavaScript", ".ts": "TypeScript",
    ".jsx": "React (JSX)", ".tsx": "React (TSX)",
    ".go": "Go", ".rs": "Rust", ".rb": "Ruby",
    ".php": "PHP", ".cs": "C#", ".cpp": "C++",
    ".c": "C", ".h": "C/C++ Header", ".swift": "Swift",
    ".scala": "Scala",
}


class ContextGenerator:
    """Generates Claude-optimized context from Doug's indexed data.

    Produces context documents, CLAUDE.md files, and architecture maps
    from previously indexed repository caches.
    """

    def __init__(self, config: Optional[DougConfig] = None):
        self.config = config or DougConfig()
        self.query_tool = AIQueryTool(config=self.config)

    @staticmethod
    def estimate_tokens(text: str) -> int:
        """Estimate token count for a text string.

        Uses a simple heuristic of ~4 characters per token,
        which is reasonably accurate for English text and code.
        """
        return len(text) // 4

    # ─── Context Document ────────────────────────────────────────────

    def generate_context_document(
        self,
        repos: Optional[List[str]] = None,
        max_tokens: Optional[int] = None,
    ) -> str:
        """Generate a comprehensive context document for AI conversations.

        Args:
            repos: Specific repos to include (default: all indexed).
            max_tokens: Approximate token budget. Sections are progressively
                        truncated to fit.

        Returns:
            Markdown-formatted context document.
        """
        repo_names = repos or self.query_tool.list_repos()
        if not repo_names:
            return "No repositories indexed. Run 'doug index' first."

        global_index = self.query_tool._load_global_index()

        sections: List[str] = []

        # Header
        total_files = global_index.get("total_files", 0) if global_index else 0
        total_apis = global_index.get("total_apis", 0) if global_index else 0
        sections.append(
            f"# Project Context for AI Assistant\n\n"
            f"Generated by Doug v{__version__} on "
            f"{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}\n"
            f"Repositories: {len(repo_names)} | "
            f"Source Files: {total_files} | "
            f"API Endpoints: {total_apis}\n"
        )

        # Per-repo sections
        for name in repo_names:
            repo_section = self._build_repo_section(name)
            if repo_section:
                sections.append(repo_section)

        # Cross-repo API map
        all_apis = self.query_tool.list_apis()
        if all_apis:
            sections.append(self._build_api_map_section(all_apis))

        document = "\n---\n\n".join(sections)

        if max_tokens:
            document = self._truncate_to_budget(document, max_tokens)

        return document

    def _build_repo_section(self, repo_name: str) -> Optional[str]:
        """Build a context section for a single repository."""
        data = self.query_tool._load_repo_cache(repo_name)
        if not data:
            return None

        summary = data.get("summary", {})
        build = data.get("build", {})
        apis = data.get("apis", [])
        readme = data.get("readme", "")

        lines = [f"## Repository: {repo_name}\n"]

        # Summary stats
        build_type = build.get("type", "unknown")
        lines.append(
            f"**Build:** {build_type} | "
            f"**Files:** {summary.get('total_files', 0)} | "
            f"**Source:** {summary.get('source_files', 0)} | "
            f"**APIs:** {len(apis)}\n"
        )

        # Key components
        for section_name in ("controllers", "services", "models"):
            items = data.get(section_name, [])
            if items:
                names = [i.get("class") or i.get("name", "?") for i in items[:10]]
                suffix = f" (+{len(items) - 10} more)" if len(items) > 10 else ""
                lines.append(
                    f"**{section_name.title()}:** {', '.join(names)}{suffix}"
                )

        # API endpoints
        if apis:
            lines.append("\n### API Endpoints\n")
            lines.append("| Method | Path | File |")
            lines.append("|--------|------|------|")
            for api in apis[:30]:
                lines.append(
                    f"| {api.get('method', '?')} | {api.get('path', '?')} "
                    f"| {api.get('file', '?')} |"
                )
            if len(apis) > 30:
                lines.append(f"\n*...and {len(apis) - 30} more endpoints*")

        # Key dependencies
        deps = build.get("dependencies", [])
        if deps:
            dep_names = [d.get("name", "?") for d in deps[:15]]
            lines.append(f"\n**Key Dependencies:** {', '.join(dep_names)}")

        # README excerpt
        if readme:
            excerpt = self._extract_first_paragraph(readme)
            if excerpt and excerpt != "No description available.":
                lines.append(f"\n### README Excerpt\n\n{excerpt}")

        return "\n".join(lines)

    def _build_api_map_section(self, apis: List[Dict[str, Any]]) -> str:
        """Build a cross-repo API domain map."""
        lines = ["## Cross-Repository API Map\n"]

        # Group by path prefix (first two segments)
        domains: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for api in apis:
            path = api.get("path", "/")
            parts = [p for p in path.split("/") if p]
            prefix = "/" + "/".join(parts[:2]) if len(parts) >= 2 else path
            domains[prefix].append(api)

        for prefix in sorted(domains):
            lines.append(f"**{prefix}**")
            for api in domains[prefix][:10]:
                repo = api.get("repo", "?")
                lines.append(
                    f"  {api.get('method', '?'):6s} {api.get('path', '?')}"
                    f"  [{repo}]"
                )
            if len(domains[prefix]) > 10:
                lines.append(f"  ...and {len(domains[prefix]) - 10} more")
            lines.append("")

        return "\n".join(lines)

    def _truncate_to_budget(self, document: str, max_tokens: int) -> str:
        """Progressively truncate a document to fit a token budget."""
        if self.estimate_tokens(document) <= max_tokens:
            return document

        # Strategy: split into sections and trim from the bottom
        sections = document.split("\n---\n\n")
        while len(sections) > 1 and self.estimate_tokens("\n---\n\n".join(sections)) > max_tokens:
            sections.pop()

        result = "\n---\n\n".join(sections)

        # If still over budget, hard-truncate
        target_chars = max_tokens * 4
        if len(result) > target_chars:
            result = result[:target_chars] + "\n\n*[Truncated to fit token budget]*"

        return result

    # ─── CLAUDE.md Generation ────────────────────────────────────────

    def generate_claude_md(self, repo_name: str) -> str:
        """Generate a CLAUDE.md file for a specific repository.

        Args:
            repo_name: Name of the indexed repository.

        Returns:
            CLAUDE.md content as a string.
        """
        data = self.query_tool._load_repo_cache(repo_name)
        if not data:
            return f"# Error\n\nRepository not found: {repo_name}"

        summary = data.get("summary", {})
        build = data.get("build", {})
        apis = data.get("apis", [])
        readme = data.get("readme", "")
        structure = data.get("structure", {})
        conventions = self._infer_conventions(data)

        lines = [
            "# CLAUDE.md",
            "",
            "<!-- Auto-generated by Doug. Edit as needed. -->",
            "",
            "This file provides guidance to Claude Code when working "
            "in this repository.",
            "",
        ]

        # Project Overview
        lines.append("## Project Overview\n")
        if readme:
            # Extract first meaningful paragraph
            overview = self._extract_first_paragraph(readme)
            lines.append(overview)
        else:
            lines.append(
                f"A {conventions.get('primary_language', 'mixed-language')} project "
                f"with {summary.get('source_files', 0)} source files."
            )
        lines.append("")

        # Build & Run
        build_type = build.get("type", "unknown")
        lines.append("## Build & Run\n")
        commands = _BUILD_COMMANDS.get(build_type)
        if commands:
            lines.append("```bash")
            for label, cmd in commands.items():
                # Replace placeholder with actual repo name
                cmd = cmd.replace("<module>", repo_name)
                lines.append(f"# {label.title()}")
                lines.append(cmd)
            lines.append("```")
        else:
            lines.append(f"Build system: {build_type}")
        lines.append("")

        # Architecture
        lines.append("## Architecture\n")
        lines.append(
            f"{conventions.get('primary_language', 'Mixed-language')} "
            f"{build_type} project with {summary.get('source_files', 0)} source files.\n"
        )

        # Key directories (depth 2)
        top_dirs = self._extract_top_dirs(structure, max_depth=2)
        if top_dirs:
            lines.append("### Key Directories\n")
            lines.append("```")
            for d in top_dirs:
                lines.append(d)
            lines.append("```")
            lines.append("")

        # Key Components
        for section_name, label in [
            ("controllers", "Controllers"),
            ("services", "Services"),
            ("models", "Models"),
        ]:
            items = data.get(section_name, [])
            if items:
                lines.append(f"### {label} ({len(items)})\n")
                for item in items[:15]:
                    cls = item.get("class", "")
                    path = item.get("path", "")
                    if cls:
                        lines.append(f"- `{cls}` — {path}")
                    else:
                        lines.append(f"- {path}")
                if len(items) > 15:
                    lines.append(f"- *...and {len(items) - 15} more*")
                lines.append("")

        # API Endpoints
        if apis:
            lines.append(f"## API Endpoints ({len(apis)})\n")
            lines.append("| Method | Path | File |")
            lines.append("|--------|------|------|")
            for api in apis[:40]:
                lines.append(
                    f"| {api.get('method', '?')} | `{api.get('path', '?')}` "
                    f"| {api.get('file', '?')} |"
                )
            if len(apis) > 40:
                lines.append(f"\n*...and {len(apis) - 40} more endpoints*")
            lines.append("")

        # Configuration Files
        configs = data.get("configs", [])
        if configs:
            lines.append("## Configuration Files\n")
            for cfg in configs[:20]:
                lines.append(f"- {cfg.get('path', cfg.get('name', '?'))}")
            if len(configs) > 20:
                lines.append(f"- *...and {len(configs) - 20} more*")
            lines.append("")

        # Coding Conventions
        lines.append("## Coding Conventions\n")
        lines.append(f"- **Language:** {conventions.get('primary_language', 'Unknown')}")
        lines.append(f"- **Build system:** {build_type}")
        if conventions.get("naming_style"):
            lines.append(f"- **Naming:** {conventions['naming_style']}")
        if conventions.get("test_framework"):
            lines.append(f"- **Testing:** {conventions['test_framework']}")
        if conventions.get("linters"):
            lines.append(f"- **Linting:** {', '.join(conventions['linters'])}")
        lines.append("")

        return "\n".join(lines)

    def _infer_conventions(self, repo_data: Dict[str, Any]) -> Dict[str, Any]:
        """Infer coding conventions from indexed repo data."""
        conventions: Dict[str, Any] = {}

        # Primary language from file extensions
        ext_counts: Counter = Counter()
        for section in ("services", "models", "controllers"):
            for item in repo_data.get(section, []):
                path = item.get("path", "")
                ext = Path(path).suffix.lower()
                if ext:
                    ext_counts[ext] += 1

        if ext_counts:
            top_ext = ext_counts.most_common(1)[0][0]
            conventions["primary_language"] = _LANG_NAMES.get(top_ext, top_ext)
        else:
            # Fallback: infer from build type
            build_type = repo_data.get("build", {}).get("type", "")
            lang_map = {
                "gradle": "Java/Kotlin", "maven": "Java",
                "npm": "JavaScript/TypeScript", "pip": "Python",
                "go": "Go", "cargo": "Rust",
            }
            conventions["primary_language"] = lang_map.get(build_type, "Unknown")

        # Naming style from class names
        classes = []
        for section in ("services", "models", "controllers"):
            for item in repo_data.get(section, []):
                cls = item.get("class", "")
                if cls:
                    classes.append(cls)

        if classes:
            camel = sum(1 for c in classes if c[0].isupper() and "_" not in c)
            snake = sum(1 for c in classes if "_" in c)
            if camel > snake:
                conventions["naming_style"] = "PascalCase classes"
            elif snake > camel:
                conventions["naming_style"] = "snake_case"

        # Test framework from build dependencies
        deps = repo_data.get("build", {}).get("dependencies", [])
        for dep in deps:
            dep_name = dep.get("name", "").lower()
            for key, framework in _TEST_FRAMEWORKS.items():
                if key in dep_name:
                    conventions["test_framework"] = framework
                    break
            if "test_framework" in conventions:
                break

        # Linter detection from config files
        linters = []
        for cfg in repo_data.get("configs", []):
            cfg_name = Path(cfg.get("path", cfg.get("name", ""))).name
            if cfg_name in _LINTER_FILES:
                linters.append(cfg_name)
        if linters:
            conventions["linters"] = linters

        return conventions

    @staticmethod
    def _extract_first_paragraph(readme: str) -> str:
        """Extract the first meaningful paragraph from README content."""
        lines = readme.strip().split("\n")
        paragraph: List[str] = []
        started = False

        for line in lines:
            stripped = line.strip()

            # Skip headings, badges, and HTML tags
            if (
                stripped.startswith("#")
                or stripped.startswith("![")
                or stripped.startswith("<")
                or stripped.startswith("[!")
            ):
                if started:
                    break
                continue

            if stripped:
                started = True
                paragraph.append(stripped)
            elif started:
                break

        result = " ".join(paragraph)
        if len(result) > 500:
            result = result[:500] + "..."
        return result or "No description available."

    @staticmethod
    def _extract_top_dirs(structure: Dict[str, Any], max_depth: int = 2) -> List[str]:
        """Extract top-level directory listing from structure tree."""
        dirs_list: List[str] = []

        def _walk(node: Dict[str, Any], prefix: str, depth: int) -> None:
            if depth > max_depth:
                return
            for d_name in sorted(node.get("dirs", {}).keys()):
                dirs_list.append(f"{prefix}{d_name}/")
                _walk(node["dirs"][d_name], prefix + "  ", depth + 1)

        _walk(structure, "", 0)
        return dirs_list[:30]  # Cap output

    # ─── Architecture Map ────────────────────────────────────────────

    def generate_architecture_map(
        self, repos: Optional[List[str]] = None
    ) -> str:
        """Generate a cross-repository architecture map.

        Shows service topology, API domains, and shared dependencies.

        Args:
            repos: Specific repos to include (default: all indexed).

        Returns:
            Markdown-formatted architecture map.
        """
        repo_names = repos or self.query_tool.list_repos()
        if not repo_names:
            return "No repositories indexed. Run 'doug index' first."

        lines = [
            "# Architecture Map\n",
            f"Generated by Doug v{__version__} on "
            f"{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}\n",
        ]

        # Service topology
        lines.append("## Service Topology\n")
        dep_registry: Dict[str, List[str]] = defaultdict(list)

        for name in repo_names:
            data = self.query_tool._load_repo_cache(name)
            if not data:
                continue

            build = data.get("build", {})
            build_type = build.get("type", "unknown")
            api_count = len(data.get("apis", []))
            summary = data.get("summary", {})

            lines.append(
                f"**{name}** ({build_type}) — "
                f"{summary.get('source_files', 0)} source files, "
                f"{api_count} API endpoints"
            )

            # API path prefixes
            api_prefixes = set()
            for api in data.get("apis", []):
                path = api.get("path", "/")
                parts = [p for p in path.split("/") if p]
                if parts:
                    api_prefixes.add("/" + parts[0])

            if api_prefixes:
                lines.append(f"  Exposes: {', '.join(sorted(api_prefixes))}")

            # Track dependencies for cross-repo analysis
            for dep in build.get("dependencies", []):
                dep_name = dep.get("name", "")
                if dep_name:
                    dep_registry[dep_name].append(name)

            lines.append("")

        # API domain map
        all_apis = self.query_tool.list_apis()
        if all_apis:
            # Filter to requested repos
            if repos:
                all_apis = [a for a in all_apis if a.get("repo") in repos]

            lines.append("## API Domain Map\n")
            domains: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
            for api in all_apis:
                path = api.get("path", "/")
                parts = [p for p in path.split("/") if p]
                prefix = "/" + "/".join(parts[:2]) if len(parts) >= 2 else path
                domains[prefix].append(api)

            for prefix in sorted(domains):
                lines.append(f"### {prefix}\n")
                lines.append("| Method | Path | Repo |")
                lines.append("|--------|------|------|")
                for api in domains[prefix]:
                    lines.append(
                        f"| {api.get('method', '?')} | {api.get('path', '?')} "
                        f"| {api.get('repo', '?')} |"
                    )
                lines.append("")

        # Shared dependencies
        shared = {dep: dep_repos for dep, dep_repos in dep_registry.items()
                  if len(dep_repos) > 1}
        if shared:
            lines.append("## Shared Dependencies\n")
            for dep_name in sorted(shared, key=lambda d: len(shared[d]), reverse=True)[:20]:
                lines.append(f"- **{dep_name}**: {', '.join(shared[dep_name])}")
            lines.append("")

        return "\n".join(lines)
